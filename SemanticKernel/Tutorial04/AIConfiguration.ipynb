{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Configuration Options\n",
    "\n",
    "OpenAI has defined some settings you can use to change the behavior of the Large Language Model's response in both subtle and less subtle ways. Whether you aren't getting the response you expected or you think you can maybe get a slightly better response, these configuration options may be a good place to look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "#r \"nuget: Microsoft.SemanticKernel, 1.7.1\"\n",
    "#!import ../config/SettingsHelper.cs\n",
    "using Microsoft.SemanticKernel;\n",
    "\n",
    "MySettings settings = Settings.LoadFromFile();\n",
    "IKernelBuilder builder = Kernel.CreateBuilder();\n",
    "if (settings.Type == \"azure\")\n",
    "    builder.AddAzureOpenAIChatCompletion(\n",
    "        settings.AzureOpenAI.ChatDeployment, \"gpt-35-turbo\", settings.AzureOpenAI.Endpoint, settings.AzureOpenAI.ApiKey);\n",
    "else\n",
    "    builder.AddOpenAIChatCompletion(\n",
    "        settings.OpenAI.Model, settings.OpenAI.ApiKey, settings.OpenAI.OrgId);\n",
    "Kernel kernel = builder.Build();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll first need to create a PromptTemplateConfig object. Within that object, you'll need to define the Completion property and it has all of the configuration options for changing the completion generated. These options include: \n",
    "- **MaxTokens** This option is of critical importance for three reasons:\n",
    "-- each LLM model has a maximum number of tokens that can be used per API request. When estimating tokens, think 1 token is approximately 4 characters in English words. For a GPT model, the tokens in the prompt plus the tokens in the generated completion must be less than your chosen model's maximum token length. GPT-3 models are limited to 2,049 tokens, but GPT-4 can go all the way up to 32,768 tokens! Read more about models and their differences [here](https://platform.openai.com/docs/models).\n",
    "-- OpenAI also has a rate limit. This means you can only make so many requests per tokens every minute. Read more [here](https://platform.openai.com/docs/guides/rate-limits/overview).\n",
    "-- the number of tokens generated by OpenAI directly corresponds to the monetary cost to you! Read more [here](https://openai.com/pricing). \n",
    "- **Temperature** This can be set as low as 0.0 or as high as 2.0. A higher temperature allows the model to generate more \"creative\" completions. A lower temperature has the opposite effect and completions are more deterministic (though not entirely deterministic!) When generating creative works, you probably want a higher temperature, but when converting one language to another, you probably want a lower temperature.\n",
    "- **TopP** Unlike Temperature, the range of this setting is 0.0 to 1.0. When the model generates possible options for the next token, TopP limits the number of the likeliest options. In practice, TopP has a similar \"randomizing\" effect as Temperature. Both settings can be used together.\n",
    "- **FrequencyPenalty** This can be set as low as 0.0 or as high as 2.0. Setting this value higher discourages the model from repeating words in the generated completion.\n",
    "- **PresencePenalty** This can be set as low as 0.0 or as high as 2.0. Setting this value higher encourages the model to generate tokens not already generated previously. Though similar to the frequency penalty, note the difference between frequency of words and presence of tokens - where tokens are often parts of words.\n",
    "- **StopSequences** Stop sequences are a list of characters like \"\\n\" or \"###\" though you can define anything you want here. This setting can be a little confusing until you play with it. For some completions, the LLM will generate tokens until it hits max tokens, but if you want it to stop generating tokens at a certain point, you can enter stop sequences in your prompt to control this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "using Microsoft.SemanticKernel.Connectors.OpenAI;\n",
    "\n",
    "string functionDefinition = \"Write a story about: {{$input}}\";\n",
    "\n",
    "OpenAIPromptExecutionSettings executionSettings = new()\n",
    "{\n",
    "    MaxTokens = 250,\n",
    "\n",
    "    // Controls randomness. Lowering the temperature means that the model will produce more repetitive and deterministic responses. Increasing the temperature will result in more unexpected or creative responses.\n",
    "    // Try adjusting temperature or Top P but not both.\n",
    "    Temperature = 0.7, // range: 0.0 - 1.0\n",
    "\n",
    "    // Similar to temperature, this controls randomness but uses a different method. Lowering Top P will narrow the modelâ€™s token selection to likelier tokens. Increasing Top P will let the model choose from tokens with both high and low likelihood.\n",
    "    // Try adjusting temperature or Top P but not both.\n",
    "    TopP = 0.5, // range: 0.0 - 1.0\n",
    "\n",
    "    // Frequency Penalty: Reduce the chance of repeating a token proportionally based on how often it has appeared in the text so far.\n",
    "    FrequencyPenalty = 0.0, // range: 0.0 - 2.0 \n",
    "\n",
    "    // Reduce the chance of repeating any token that has appeared in the text at all so far.\n",
    "    PresencePenalty = 0.0, // range: 0.0 - 2.0\n",
    "\n",
    "    // Make the model end its response at any of the defined strings.\n",
    "    StopSequences = new List<string>() { \"###\" }, \n",
    "};\n",
    "\n",
    "// register your semantic function\n",
    "KernelFunction chatFunction = \n",
    "    kernel.CreateFunctionFromPrompt(functionDefinition, executionSettings);\n",
    "\n",
    "string prompt = \"an android on the run from a human\";\n",
    "FunctionResult result = await chatFunction.InvokeAsync(kernel, new(){{\"input\", prompt}});\n",
    "\n",
    "Console.WriteLine(result.GetValue<string>()!);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully running the code above generated an interesting reading break for you!\n",
    "\n",
    "We begin with a very simple creative writing prompt. We create a PromptTemplateConfig object and set it's Completion property where each of the previously described configuration options resides. We then have to tie it all together before we register everything as a semantic function with the kernel. (NOTE: The Chat function of the ChatBot skill is a built-in function.)\n",
    "\n",
    "The above code used an inline function, but this tutorial contains a semantic function as well. You can expand the AutomationSkill\\MakeAListFunction folders and find the config.json file there that defines the same configuration options using that style of skill. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Modify the configuration options above to see how they affect the generated completion. Here are some tips, but feel free to explore: \n",
    "- Don't raise the Max Tokens and risk hitting rate limits or spending a lot of money. Instead, lower max tokens to see how doing so changes the completion. (A max tokens of 200 or so may be a good number to test repeated runs in this exercise.)\n",
    "- Play with Temperature and TopP to see how \"creative\" the completions get! Then set both options to zero and run the same code twice. Does the result surprise you?\n",
    "- Maximize Frequency Penalty first and see how often the words 'android' and 'human' come up. How does the AI stil generate a story with these penalties in place?\n",
    "- Since Stop Sequences don't make a lot of sense in this simple creative writing scenario, we'll explore them in our tutorial on prompt engineering so don't worry about playing with them here. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
